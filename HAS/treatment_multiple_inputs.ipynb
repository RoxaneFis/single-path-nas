{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import savetxt, loadtxt, asarray\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from progressbar import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential, Model\n",
    "from random import randrange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import time\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "\n",
    "from plot_figures import save_model\n",
    "\n",
    "pbar = ProgressBar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_blocks = 37 #(36 + 1 FC layer at the end)\n",
    "#max_blocks = 36\n",
    "\n",
    "nb_param =7\n",
    "#nb_param =13\n",
    "nb_hw_param = 12\n",
    "#nb_param =12\n",
    "\n",
    "inversed = True\n",
    "only_inversed = False\n",
    "\n",
    "if inversed and not only_inversed:\n",
    "    path_entire_model= \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/results_best_models/multiply/with_inversed_hw\"\n",
    "elif only_inversed:\n",
    "    path_entire_model= \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/results_best_models/multiply/only_inversed_hw\"\n",
    "else : \n",
    "    path_entire_model= \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/results_best_models/multiply/no_inversed_hw\"\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "DATASET_SIZE = 168308\n",
    "train_size = int(0.90 * DATASET_SIZE) #135 805\n",
    "test_size = int(0.10 * DATASET_SIZE) # 15 089\n",
    "\n",
    "nb_training_batches = train_size //BATCH_SIZE +1\n",
    "nb_test_batches = test_size //BATCH_SIZE +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def from_np_array(array_string):\n",
    "    array_string = ','.join(array_string.replace('[ ', '[').split())\n",
    "    return np.asarray(ast.literal_eval(array_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Proccessed Neural Networks (without HyperParameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pycache\n",
    "path_raw_val = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/good_data/parsed_nondups_val_3.csv'\n",
    "path_raw_train = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/good_data/parsed_nondups_train_3.csv'\n",
    "\n",
    "#path_processed_val_nn = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/val_13.csv'\n",
    "#path_processed_train_nn = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/train_13.csv'\n",
    "\n",
    "\n",
    "path_processed_val_nn = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/good_data/val7_from1600.csv'\n",
    "path_processed_train_nn = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/good_data/train7_from1600.csv'\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power < 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "path_raw_val = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/lower_power/parsed_nondups_val_lower_power.csv'\n",
    "path_raw_train = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/lower_power/parsed_nondups_train_lower_power.csv'\n",
    "\n",
    "path_processed_val_nn = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/lower_power/val_low_power.csv'\n",
    "path_processed_train_nn = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/lower_power/train_low_power.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total time : 1.5255195666666665 min\n"
    }
   ],
   "source": [
    "#%pycache\n",
    "tin= time.clock()\n",
    "\n",
    "    \n",
    "val = pd.read_csv(path_processed_val_nn,converters={'NN_dataframe': from_np_array})\n",
    "train = pd.read_csv(path_processed_train_nn,converters={'NN_dataframe': from_np_array})\n",
    "\n",
    "tfin= time.clock()\n",
    "print(f'Total time : {(tfin-tin)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete to zeros (max shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(arr):\n",
    "    return arr.shape[0]\n",
    "\n",
    "def add_zero_blocks(arr):\n",
    "    zero_blocks = np.zeros((max_blocks-arr.shape[0],arr.shape[1]))\n",
    "    return np.append(arr, zero_blocks, axis=0)\n",
    "\n",
    "val[\"nb_blocks\"] = val[\"NN_dataframe\"].apply(lambda x : get_shape(x))\n",
    "train[\"nb_blocks\"] = train[\"NN_dataframe\"].apply(lambda x : get_shape(x))\n",
    "\n",
    "\n",
    "val['NN_dataframe']= val['NN_dataframe'].apply(lambda x : add_zero_blocks(x))\n",
    "train['NN_dataframe']= train['NN_dataframe'].apply(lambda x : add_zero_blocks(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Get y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To retrieve y_train / y_test\n",
    "\n",
    "train_hw = pd.read_csv(path_raw_train)\n",
    "val_hw = pd.read_csv(path_raw_val)\n",
    "\n",
    "y_train = np.array(train_hw[\"total_power\"].tolist())\n",
    "y_train=y_train.reshape(y_train.shape[0],-1)\n",
    "\n",
    "y_val = np.array(val_hw[\"total_power\"].tolist())\n",
    "y_val=y_val.reshape(y_val.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mac_num  mac_array_num  data_bits  sram_size  max_filter_size  \\\n0    118.0            2.0      256.0   179968.0           2048.0   \n1     87.0            2.0      512.0    99968.0           2048.0   \n2    124.0            2.0      512.0    99968.0           3072.0   \n3     80.0            2.0     1024.0    99968.0           1024.0   \n4     86.0            2.0      512.0    60000.0           1536.0   \n\n              G           C           B         J  name    bw_power  \\\n0  3.636011e+06  18770712.0  19987780.0  0.000533   0.0   79.951120   \n1  2.325259e+06  14362735.0  21849702.0  0.000570   0.0   87.398808   \n2  2.533459e+06  12739050.0  21849702.0  0.000543   0.0   87.398808   \n3  2.327509e+06  12155422.0  21849702.0  0.000684   0.0   87.398808   \n4  1.728057e+06  16407889.0  28402776.0  0.000604   0.0  113.611104   \n\n   core_power  total_power  \n0   21.330414   101.281534  \n1   22.816285   110.215093  \n2   21.716202   109.115010  \n3   27.343500   114.742308  \n4   24.174307   137.785411  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mac_num</th>\n      <th>mac_array_num</th>\n      <th>data_bits</th>\n      <th>sram_size</th>\n      <th>max_filter_size</th>\n      <th>G</th>\n      <th>C</th>\n      <th>B</th>\n      <th>J</th>\n      <th>name</th>\n      <th>bw_power</th>\n      <th>core_power</th>\n      <th>total_power</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>118.0</td>\n      <td>2.0</td>\n      <td>256.0</td>\n      <td>179968.0</td>\n      <td>2048.0</td>\n      <td>3.636011e+06</td>\n      <td>18770712.0</td>\n      <td>19987780.0</td>\n      <td>0.000533</td>\n      <td>0.0</td>\n      <td>79.951120</td>\n      <td>21.330414</td>\n      <td>101.281534</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>87.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>99968.0</td>\n      <td>2048.0</td>\n      <td>2.325259e+06</td>\n      <td>14362735.0</td>\n      <td>21849702.0</td>\n      <td>0.000570</td>\n      <td>0.0</td>\n      <td>87.398808</td>\n      <td>22.816285</td>\n      <td>110.215093</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>124.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>99968.0</td>\n      <td>3072.0</td>\n      <td>2.533459e+06</td>\n      <td>12739050.0</td>\n      <td>21849702.0</td>\n      <td>0.000543</td>\n      <td>0.0</td>\n      <td>87.398808</td>\n      <td>21.716202</td>\n      <td>109.115010</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80.0</td>\n      <td>2.0</td>\n      <td>1024.0</td>\n      <td>99968.0</td>\n      <td>1024.0</td>\n      <td>2.327509e+06</td>\n      <td>12155422.0</td>\n      <td>21849702.0</td>\n      <td>0.000684</td>\n      <td>0.0</td>\n      <td>87.398808</td>\n      <td>27.343500</td>\n      <td>114.742308</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>60000.0</td>\n      <td>1536.0</td>\n      <td>1.728057e+06</td>\n      <td>16407889.0</td>\n      <td>28402776.0</td>\n      <td>0.000604</td>\n      <td>0.0</td>\n      <td>113.611104</td>\n      <td>24.174307</td>\n      <td>137.785411</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_hw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x1a32f53518>"
     },
     "metadata": {},
     "execution_count": 10
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 400.041317 248.518125\" width=\"400.041317pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 400.041317 248.518125 \nL 400.041317 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.375 224.64 \nL 387.175 224.64 \nL 387.175 7.2 \nL 52.375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 67.593182 224.64 \nL 98.029545 224.64 \nL 98.029545 17.554286 \nL 67.593182 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 98.029545 224.64 \nL 128.465909 224.64 \nL 128.465909 223.211253 \nL 98.029545 223.211253 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 128.465909 224.64 \nL 158.902273 224.64 \nL 158.902273 224.275772 \nL 128.465909 224.275772 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 158.902273 224.64 \nL 189.338636 224.64 \nL 189.338636 224.450601 \nL 158.902273 224.450601 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 189.338636 224.64 \nL 219.775 224.64 \nL 219.775 224.593379 \nL 189.338636 224.593379 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 219.775 224.64 \nL 250.211364 224.64 \nL 250.211364 224.535102 \nL 219.775 224.535102 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 250.211364 224.64 \nL 280.647727 224.64 \nL 280.647727 224.617661 \nL 250.211364 224.617661 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 280.647727 224.64 \nL 311.084091 224.64 \nL 311.084091 224.628345 \nL 280.647727 224.628345 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 311.084091 224.64 \nL 341.520455 224.64 \nL 341.520455 224.638057 \nL 311.084091 224.638057 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p46d1609034)\" d=\"M 341.520455 224.64 \nL 371.956818 224.64 \nL 371.956818 224.639029 \nL 341.520455 224.639029 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 67.553343 224.64 \nL 67.553343 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m98a97f9982\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"67.553343\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(64.372093 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 106.226058 224.64 \nL 106.226058 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.226058\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2000 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(93.501058 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 144.898774 224.64 \nL 144.898774 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.898774\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4000 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(132.173774 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 183.571489 224.64 \nL 183.571489 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"183.571489\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6000 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(170.846489 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 222.244205 224.64 \nL 222.244205 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"222.244205\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8000 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(209.519205 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 260.91692 224.64 \nL 260.91692 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"260.91692\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10000 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(245.01067 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 299.589636 224.64 \nL 299.589636 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.589636\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 12000 -->\n      <g transform=\"translate(283.683386 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 338.262351 224.64 \nL 338.262351 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"338.262351\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 14000 -->\n      <g transform=\"translate(322.356101 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 376.935067 224.64 \nL 376.935067 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.935067\" xlink:href=\"#m98a97f9982\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 16000 -->\n      <g transform=\"translate(361.028817 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 224.64 \nL 387.175 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mcb600ebd3a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(39.0125 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 200.358105 \nL 387.175 200.358105 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"200.358105\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 25000 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(13.5625 204.157324)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 176.07621 \nL 387.175 176.07621 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"176.07621\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50000 -->\n      <g transform=\"translate(13.5625 179.875429)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 151.794315 \nL 387.175 151.794315 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"151.794315\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 75000 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(13.5625 155.593533)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 127.51242 \nL 387.175 127.51242 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"127.51242\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 100000 -->\n      <g transform=\"translate(7.2 131.311638)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 103.230524 \nL 387.175 103.230524 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"103.230524\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 125000 -->\n      <g transform=\"translate(7.2 107.029743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 78.948629 \nL 387.175 78.948629 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"78.948629\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 150000 -->\n      <g transform=\"translate(7.2 82.747848)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 54.666734 \nL 387.175 54.666734 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"54.666734\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 175000 -->\n      <g transform=\"translate(7.2 58.465953)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_35\">\n      <path clip-path=\"url(#p46d1609034)\" d=\"M 52.375 30.384839 \nL 387.175 30.384839 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#mcb600ebd3a\" y=\"30.384839\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 200000 -->\n      <g transform=\"translate(7.2 34.184058)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 52.375 224.64 \nL 52.375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 387.175 224.64 \nL 387.175 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 52.375 224.64 \nL 387.175 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 52.375 7.2 \nL 387.175 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p46d1609034\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"52.375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "train_hw[\"total_power\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "198167"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(train_hw[\"total_power\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# HW PARAM ALONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mac_num  mac_array_num  data_bits  sram_size  max_filter_size  tot_mac\n0    118.0            2.0      256.0   179968.0           2048.0    236.0\n1     87.0            2.0      512.0    99968.0           2048.0    174.0\n2    124.0            2.0      512.0    99968.0           3072.0    248.0\n3     80.0            2.0     1024.0    99968.0           1024.0    160.0\n4     86.0            2.0      512.0    60000.0           1536.0    172.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mac_num</th>\n      <th>mac_array_num</th>\n      <th>data_bits</th>\n      <th>sram_size</th>\n      <th>max_filter_size</th>\n      <th>tot_mac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>118.0</td>\n      <td>2.0</td>\n      <td>256.0</td>\n      <td>179968.0</td>\n      <td>2048.0</td>\n      <td>236.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>87.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>99968.0</td>\n      <td>2048.0</td>\n      <td>174.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>124.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>99968.0</td>\n      <td>3072.0</td>\n      <td>248.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80.0</td>\n      <td>2.0</td>\n      <td>1024.0</td>\n      <td>99968.0</td>\n      <td>1024.0</td>\n      <td>160.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>60000.0</td>\n      <td>1536.0</td>\n      <td>172.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "X_train_hw_param  = train_hw[['mac_num', 'mac_array_num', 'data_bits', 'sram_size', 'max_filter_size']]\n",
    "X_train_hw_param['tot_mac'] = X_train_hw_param['mac_num']*X_train_hw_param['mac_array_num']\n",
    "\n",
    "X_val_hw_param  = val_hw[['mac_num', 'mac_array_num', 'data_bits', 'sram_size', 'max_filter_size']]\n",
    "X_val_hw_param['tot_mac'] = X_val_hw_param['mac_num']*X_val_hw_param['mac_array_num']\n",
    "\n",
    "X_train_hw_param.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes of the blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shape = np.array(train[\"nb_blocks\"])\n",
    "X_val_shape = np.array(val[\"nb_blocks\"])\n",
    "\n",
    "#X_val_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 215445/215445 [00:00<00:00, 434211.88it/s]\n100%|██████████| 215445/215445 [00:00<00:00, 738032.50it/s]\n100%|██████████| 215445/215445 [00:00<00:00, 778175.77it/s]\n100%|██████████| 215445/215445 [00:00<00:00, 778341.33it/s]\n100%|██████████| 215445/215445 [00:00<00:00, 740027.03it/s]\n100%|██████████| 215445/215445 [00:00<00:00, 735244.39it/s]\n100%|██████████| 24274/24274 [00:00<00:00, 548231.04it/s]\n100%|██████████| 24274/24274 [00:00<00:00, 453896.14it/s]\n100%|██████████| 24274/24274 [00:00<00:00, 462513.05it/s]\n100%|██████████| 24274/24274 [00:00<00:00, 538525.40it/s]\n100%|██████████| 24274/24274 [00:00<00:00, 589305.45it/s]\n100%|██████████| 24274/24274 [00:00<00:00, 490454.82it/s]\n"
    }
   ],
   "source": [
    "def inv(x):\n",
    "    return 1/(1+x)\n",
    "\n",
    "\n",
    "if inversed : \n",
    "#nb_hw_param = 6\n",
    "    nb_hw_param = 12\n",
    "    X_train_hw_param['1/mac_num'] = X_train_hw_param['mac_num'].progress_apply(lambda x : inv(x))\n",
    "    X_train_hw_param['1/mac_array_num'] = X_train_hw_param['mac_array_num'].progress_apply(lambda x : inv(x))\n",
    "    X_train_hw_param['1/data_bits'] = X_train_hw_param['data_bits'].progress_apply(lambda x : inv(x))\n",
    "    X_train_hw_param['1/sram_size'] = X_train_hw_param['sram_size'].progress_apply(lambda x : inv(x))\n",
    "    X_train_hw_param['1/max_filter_size'] = X_train_hw_param['max_filter_size'].progress_apply(lambda x : inv(x))\n",
    "    X_train_hw_param['1/tot_mac'] = X_train_hw_param['tot_mac'].progress_apply(lambda x : inv(x))\n",
    "\n",
    "\n",
    "    X_val_hw_param['1/mac_num'] = X_val_hw_param['mac_num'].progress_apply(lambda x : inv(x))\n",
    "    X_val_hw_param['1/mac_array_num'] = X_val_hw_param['mac_array_num'].progress_apply(lambda x : inv(x))\n",
    "    X_val_hw_param['1/data_bits'] = X_val_hw_param['data_bits'].progress_apply(lambda x : inv(x))\n",
    "    X_val_hw_param['1/sram_size'] = X_val_hw_param['sram_size'].progress_apply(lambda x : inv(x))\n",
    "    X_val_hw_param['1/max_filter_size'] = X_val_hw_param['max_filter_size'].progress_apply(lambda x : inv(x))\n",
    "    X_val_hw_param['1/tot_mac'] = X_val_hw_param['tot_mac'].progress_apply(lambda x : inv(x))\n",
    "\n",
    "if only_inversed : \n",
    "    nb_hw_param = 6\n",
    "    inv = ['1/mac_num','1/mac_array_num','1/data_bits','1/sram_size','1/max_filter_size','1/tot_mac']\n",
    "    X_train_hw_param =X_train_hw_param.loc[:, inv]\n",
    "    X_val_hw_param =X_val_hw_param.loc[:, inv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mac_num  mac_array_num  data_bits  sram_size  max_filter_size  tot_mac  \\\n0     37.0            2.0      128.0   199936.0           4098.0     74.0   \n1     91.0            2.0      256.0   119936.0           2048.0    182.0   \n2     74.0            2.0      256.0   139904.0           1024.0    148.0   \n3     70.0            2.0      512.0    80000.0            512.0    140.0   \n4    103.0            2.0      256.0   199936.0           4098.0    206.0   \n\n   1/mac_num  1/mac_array_num  1/data_bits  1/sram_size  1/max_filter_size  \\\n0   0.026316         0.333333     0.007752     0.000005           0.000244   \n1   0.010870         0.333333     0.003891     0.000008           0.000488   \n2   0.013333         0.333333     0.003891     0.000007           0.000976   \n3   0.014085         0.333333     0.001949     0.000012           0.001949   \n4   0.009615         0.333333     0.003891     0.000005           0.000244   \n\n   1/tot_mac  \n0   0.013333  \n1   0.005464  \n2   0.006711  \n3   0.007092  \n4   0.004831  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mac_num</th>\n      <th>mac_array_num</th>\n      <th>data_bits</th>\n      <th>sram_size</th>\n      <th>max_filter_size</th>\n      <th>tot_mac</th>\n      <th>1/mac_num</th>\n      <th>1/mac_array_num</th>\n      <th>1/data_bits</th>\n      <th>1/sram_size</th>\n      <th>1/max_filter_size</th>\n      <th>1/tot_mac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37.0</td>\n      <td>2.0</td>\n      <td>128.0</td>\n      <td>199936.0</td>\n      <td>4098.0</td>\n      <td>74.0</td>\n      <td>0.026316</td>\n      <td>0.333333</td>\n      <td>0.007752</td>\n      <td>0.000005</td>\n      <td>0.000244</td>\n      <td>0.013333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>91.0</td>\n      <td>2.0</td>\n      <td>256.0</td>\n      <td>119936.0</td>\n      <td>2048.0</td>\n      <td>182.0</td>\n      <td>0.010870</td>\n      <td>0.333333</td>\n      <td>0.003891</td>\n      <td>0.000008</td>\n      <td>0.000488</td>\n      <td>0.005464</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74.0</td>\n      <td>2.0</td>\n      <td>256.0</td>\n      <td>139904.0</td>\n      <td>1024.0</td>\n      <td>148.0</td>\n      <td>0.013333</td>\n      <td>0.333333</td>\n      <td>0.003891</td>\n      <td>0.000007</td>\n      <td>0.000976</td>\n      <td>0.006711</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>80000.0</td>\n      <td>512.0</td>\n      <td>140.0</td>\n      <td>0.014085</td>\n      <td>0.333333</td>\n      <td>0.001949</td>\n      <td>0.000012</td>\n      <td>0.001949</td>\n      <td>0.007092</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>103.0</td>\n      <td>2.0</td>\n      <td>256.0</td>\n      <td>199936.0</td>\n      <td>4098.0</td>\n      <td>206.0</td>\n      <td>0.009615</td>\n      <td>0.333333</td>\n      <td>0.003891</td>\n      <td>0.000005</td>\n      <td>0.000244</td>\n      <td>0.004831</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "X_val_hw_param.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "#%pycache\n",
    "# Process\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_hw_param)\n",
    "X_train_hw_param_norm= scaler.transform(X_train_hw_param)\n",
    "X_val_hw_param_norm = scaler.transform(X_val_hw_param )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0         1         2         3         4         5         6  \\\n0  0.855641 -0.489793 -0.647015  0.529144  0.332994 -0.239695 -0.662872   \n1  0.300127 -0.489793 -0.258452 -0.373136  0.332994 -0.306012 -0.581406   \n2  0.963160 -0.489793 -0.258452 -0.373136  1.196488 -0.226860 -0.673972   \n3  0.174689 -0.489793  0.518673 -0.373136 -0.530500 -0.320987 -0.554381   \n4  0.282208 -0.489793 -0.258452 -0.823915 -0.098753 -0.308151 -0.577812   \n\n          7         8         9        10        11  \n0  0.706748  0.103602 -0.981619 -0.778893 -0.505309  \n1  0.706748 -0.569281 -0.066708 -0.778893 -0.411430  \n2  0.706748 -0.569281 -0.066708 -1.029986 -0.518079  \n3  0.706748 -0.906707 -0.066708 -0.026105 -0.380224  \n4  0.706748 -0.569281  1.304303 -0.527882 -0.407281  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.855641</td>\n      <td>-0.489793</td>\n      <td>-0.647015</td>\n      <td>0.529144</td>\n      <td>0.332994</td>\n      <td>-0.239695</td>\n      <td>-0.662872</td>\n      <td>0.706748</td>\n      <td>0.103602</td>\n      <td>-0.981619</td>\n      <td>-0.778893</td>\n      <td>-0.505309</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.300127</td>\n      <td>-0.489793</td>\n      <td>-0.258452</td>\n      <td>-0.373136</td>\n      <td>0.332994</td>\n      <td>-0.306012</td>\n      <td>-0.581406</td>\n      <td>0.706748</td>\n      <td>-0.569281</td>\n      <td>-0.066708</td>\n      <td>-0.778893</td>\n      <td>-0.411430</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.963160</td>\n      <td>-0.489793</td>\n      <td>-0.258452</td>\n      <td>-0.373136</td>\n      <td>1.196488</td>\n      <td>-0.226860</td>\n      <td>-0.673972</td>\n      <td>0.706748</td>\n      <td>-0.569281</td>\n      <td>-0.066708</td>\n      <td>-1.029986</td>\n      <td>-0.518079</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.174689</td>\n      <td>-0.489793</td>\n      <td>0.518673</td>\n      <td>-0.373136</td>\n      <td>-0.530500</td>\n      <td>-0.320987</td>\n      <td>-0.554381</td>\n      <td>0.706748</td>\n      <td>-0.906707</td>\n      <td>-0.066708</td>\n      <td>-0.026105</td>\n      <td>-0.380224</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.282208</td>\n      <td>-0.489793</td>\n      <td>-0.258452</td>\n      <td>-0.823915</td>\n      <td>-0.098753</td>\n      <td>-0.308151</td>\n      <td>-0.577812</td>\n      <td>0.706748</td>\n      <td>-0.569281</td>\n      <td>1.304303</td>\n      <td>-0.527882</td>\n      <td>-0.407281</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "pd.DataFrame(X_train_hw_param_norm).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mac_num  mac_array_num  data_bits  sram_size  max_filter_size  tot_mac  \\\n0    118.0            2.0      256.0   179968.0           2048.0    236.0   \n1     87.0            2.0      512.0    99968.0           2048.0    174.0   \n2    124.0            2.0      512.0    99968.0           3072.0    248.0   \n3     80.0            2.0     1024.0    99968.0           1024.0    160.0   \n4     86.0            2.0      512.0    60000.0           1536.0    172.0   \n\n   1/mac_num  1/mac_array_num  1/data_bits  1/sram_size  1/max_filter_size  \\\n0   0.008403         0.333333     0.003891     0.000006           0.000488   \n1   0.011364         0.333333     0.001949     0.000010           0.000488   \n2   0.008000         0.333333     0.001949     0.000010           0.000325   \n3   0.012346         0.333333     0.000976     0.000010           0.000976   \n4   0.011494         0.333333     0.001949     0.000017           0.000651   \n\n   1/tot_mac  \n0   0.004219  \n1   0.005714  \n2   0.004016  \n3   0.006211  \n4   0.005780  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mac_num</th>\n      <th>mac_array_num</th>\n      <th>data_bits</th>\n      <th>sram_size</th>\n      <th>max_filter_size</th>\n      <th>tot_mac</th>\n      <th>1/mac_num</th>\n      <th>1/mac_array_num</th>\n      <th>1/data_bits</th>\n      <th>1/sram_size</th>\n      <th>1/max_filter_size</th>\n      <th>1/tot_mac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>118.0</td>\n      <td>2.0</td>\n      <td>256.0</td>\n      <td>179968.0</td>\n      <td>2048.0</td>\n      <td>236.0</td>\n      <td>0.008403</td>\n      <td>0.333333</td>\n      <td>0.003891</td>\n      <td>0.000006</td>\n      <td>0.000488</td>\n      <td>0.004219</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>87.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>99968.0</td>\n      <td>2048.0</td>\n      <td>174.0</td>\n      <td>0.011364</td>\n      <td>0.333333</td>\n      <td>0.001949</td>\n      <td>0.000010</td>\n      <td>0.000488</td>\n      <td>0.005714</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>124.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>99968.0</td>\n      <td>3072.0</td>\n      <td>248.0</td>\n      <td>0.008000</td>\n      <td>0.333333</td>\n      <td>0.001949</td>\n      <td>0.000010</td>\n      <td>0.000325</td>\n      <td>0.004016</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80.0</td>\n      <td>2.0</td>\n      <td>1024.0</td>\n      <td>99968.0</td>\n      <td>1024.0</td>\n      <td>160.0</td>\n      <td>0.012346</td>\n      <td>0.333333</td>\n      <td>0.000976</td>\n      <td>0.000010</td>\n      <td>0.000976</td>\n      <td>0.006211</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86.0</td>\n      <td>2.0</td>\n      <td>512.0</td>\n      <td>60000.0</td>\n      <td>1536.0</td>\n      <td>172.0</td>\n      <td>0.011494</td>\n      <td>0.333333</td>\n      <td>0.001949</td>\n      <td>0.000017</td>\n      <td>0.000651</td>\n      <td>0.005780</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "pd.DataFrame(X_train_hw_param).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for HW param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "X_hw_norm = np.concatenate((X_train_hw_param_norm, X_val_hw_param_norm))\n",
    "X_hw = np.concatenate((X_train_hw_param, X_val_hw_param))\n",
    "y = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Mix train/test\n",
    "#NORMED\n",
    "dataset_hw_norm = tf.data.Dataset.from_tensor_slices((X_hw_norm, y))\n",
    "dataset_hw_norm= dataset_hw_norm.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "#UNORMED\n",
    "dataset_hw = tf.data.Dataset.from_tensor_slices((X_hw, y))\n",
    "#small_dataset_hw = dataset_hw.filter(lambda x, label: label<100)\n",
    "dataset_hw= dataset_hw.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "# Split them\n",
    "#NORMED\n",
    "train_dataset_hw_norm =  dataset_hw_norm.take(train_size).batch(BATCH_SIZE)\n",
    "test_dataset_hw_norm = dataset_hw_norm.skip(train_size).batch(BATCH_SIZE)\n",
    "\n",
    "#UNORMODED\n",
    "train_dataset_hw =  dataset_hw.take(train_size).batch(BATCH_SIZE)\n",
    "test_dataset_hw = dataset_hw.skip(train_size).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "# NN TREATEMENT ALONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_nn =np.array(train['NN_dataframe'].tolist())\n",
    "X_val_nn = np.array(val['NN_dataframe'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divde columns by standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "means = []\n",
    "std = []\n",
    "X_train_nn_norm=X_train_nn.copy()\n",
    "X_val_nn_norm = X_val_nn.copy()\n",
    "for i in range(nb_param):\n",
    "    #means.append(np.mean(X_train_nn[:,:,i]))\n",
    "    #X_train_nn[:,:,i]=-means[i]\n",
    "    std.append(np.std(X_train_nn[:,:,i]))\n",
    "    if std[i]!=0:\n",
    "        X_train_nn_norm[:,:,i]/= std[i]\n",
    "        X_val_nn_norm[:,:,i]/= std[i]  \n",
    "\n",
    "#savetxt('std.csv', std, delimiter=',')\n",
    "#data = loadtxt('std.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[277884802.5673081,\n 461103.7972515926,\n 136894.47844820443,\n 138975.80512235846,\n 311.8400298266541,\n 12.189058828019654,\n 0.394164309098097]"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0         1         2         3         4         5    6\n0  0.021939  0.000547  1.099591  0.812343  0.009620  0.738367  0.0\n1  0.007313  0.000195  0.824694  0.812343  0.028861  0.738367  0.0\n2  0.006500  0.000173  0.824694  0.722083  0.028861  0.082041  0.0\n3  0.201509  0.004908  0.733061  2.798070  0.025654  0.738367  0.0\n4  0.906793  0.022409  2.840611  3.700673  0.397640  0.738367  0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.021939</td>\n      <td>0.000547</td>\n      <td>1.099591</td>\n      <td>0.812343</td>\n      <td>0.009620</td>\n      <td>0.738367</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.007313</td>\n      <td>0.000195</td>\n      <td>0.824694</td>\n      <td>0.812343</td>\n      <td>0.028861</td>\n      <td>0.738367</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.006500</td>\n      <td>0.000173</td>\n      <td>0.824694</td>\n      <td>0.722083</td>\n      <td>0.028861</td>\n      <td>0.082041</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.201509</td>\n      <td>0.004908</td>\n      <td>0.733061</td>\n      <td>2.798070</td>\n      <td>0.025654</td>\n      <td>0.738367</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.906793</td>\n      <td>0.022409</td>\n      <td>2.840611</td>\n      <td>3.700673</td>\n      <td>0.397640</td>\n      <td>0.738367</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "pd.DataFrame(X_train_nn_norm[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "X_nn_norm = np.concatenate((X_train_nn_norm, X_val_nn_norm))\n",
    "X_nn = np.concatenate((X_train_nn, X_val_nn))\n",
    "y = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Mix test/train \n",
    "#NORMED\n",
    "dataset_nn_norm = tf.data.Dataset.from_tensor_slices((X_nn_norm, y))\n",
    "dataset_nn_norm= dataset_nn_norm.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "#UNNORMED\n",
    "dataset_nn = tf.data.Dataset.from_tensor_slices((X_nn, y))\n",
    "dataset_nn.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "\n",
    "# Split them\n",
    "#NORMED\n",
    "test_dataset_nn_norm = dataset_nn_norm.take(test_size).batch(BATCH_SIZE)\n",
    "train_dataset_nn_norm = dataset_nn_norm.skip(test_size).batch(BATCH_SIZE)\n",
    "\n",
    "#UNORMED\n",
    "test_dataset_nn= dataset_nn.take(test_size).batch(BATCH_SIZE)\n",
    "train_dataset_nn = dataset_nn.skip(test_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=np.array(list(zip(X_train_nn_norm, X_train_shape)))\n",
    "X_train_nn_norm_zip =  np.array( [(one,two) for one, two in zip(X_train_nn_norm, X_train_shape)]   )\n",
    "X_val_nn_norm_zip =  np.array( [(one,two) for one, two in zip(X_val_nn_norm, X_val_shape)]   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "type(X_train_hw_param_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "type(X_train_nn_norm_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Dataset (HW_params + NN arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blocks = []\n",
    "for sha in X_train_shape:\n",
    "    ones = np.ones((sha, 32))\n",
    "    zeros = np.zeros((37-sha,32))\n",
    "    train_blocks.append(np.concatenate((ones,zeros)))\n",
    "train_blocks = np.array(train_blocks)\n",
    "\n",
    "\n",
    "val_blocks = []\n",
    "for sha in X_val_shape:\n",
    "    ones = np.ones((sha, 32))\n",
    "    zeros = np.zeros((37-sha,32))\n",
    "    val_blocks.append(np.concatenate((ones,zeros)))\n",
    "val_blocks = np.array(val_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMED\n",
    "#train_dataset_norm = tf.data.Dataset.from_tensor_slices(((X_train_nn_norm, X_train_hw_param_norm), y_train))\n",
    "#test_dataset_norm = tf.data.Dataset.from_tensor_slices(((X_val_nn_norm, X_val_hw_param_norm), y_val))\n",
    "\n",
    "train_dataset_norm = tf.data.Dataset.from_tensor_slices(( (train_blocks,X_train_nn_norm, X_train_hw_param_norm), y_train) )\n",
    "test_dataset_norm = tf.data.Dataset.from_tensor_slices(( (val_blocks,X_val_nn_norm, X_val_hw_param_norm), y_val) )\n",
    "\n",
    "train_dataset_norm = train_dataset_norm.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "test_dataset_norm = test_dataset_norm.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "train_dataset_norm = train_dataset_norm.batch(BATCH_SIZE)\n",
    "test_dataset_norm = test_dataset_norm.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get loss/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_test = 'model_1_plus'\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "sha = X_train_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(215445, 37, 32)"
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "source": [
    "train_blocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = tf.Variable(np.ones((37,last_layer)),shape=(37,last_layer),name=\"a\")\n",
    "last_layer=32\n",
    "shape =tf.convert_to_tensor(np.array(X_val_shape))[10000]\n",
    "ind =[[i] for i in range(shape)]\n",
    "ones_blocks = tf.gather_nd(zeros,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "keras_input = tf.keras.layers.Input([1],dtype='int64')\n",
    "keras_input\n",
    "\n",
    "#shape =tf.convert_to_tensor(np.array(X_val_shape))[10000]\n",
    "zeros= tf.Variable(np.zeros((37-(keras_input), last_layer), dtype=np.float32), name='vector_zeros')\n",
    "#ones =tf.Variable(np.ones((shape, last_layer), dtype=np.float32), name='vector_ones')\n",
    "block = tf.concat([ones,zeros],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'multiply_22/Identity:0' shape=(None, 37, 32) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "source": [
    "tf.keras.layers.multiply([output_nn, input_blocks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PARAMS : 15809\n"
    }
   ],
   "source": [
    "last_layer = 32\n",
    "input_blocks = Input(shape=(max_blocks, last_layer),dtype='float32', name='blocks')\n",
    "input_nn = Input(shape=(max_blocks, nb_param), dtype='float32', name='input_nn')\n",
    "\n",
    "\n",
    "output_nn = layers.Conv1D(128, (2), activation='relu', padding=\"same\")(input_nn)\n",
    "output_nn = layers.Conv1D(last_layer , (2), activation='relu', padding=\"same\")(output_nn)\n",
    "output_nn = tf.keras.layers.multiply([output_nn, input_blocks])\n",
    "#output_nn =layers.Lambda(lambda x: x * input_blocks)(output_nn)\n",
    "output_nn =tf.keras.layers.Lambda( lambda x: K.sum(x, axis=1))(output_nn)\n",
    "\n",
    "\n",
    "model_hw = tf.keras.Sequential([\n",
    "     layers.Dense(32, activation='relu', input_shape=(nb_hw_param,)),\n",
    "     layers.Dense(last_layer , activation='linear')\n",
    " ])\n",
    "\n",
    "#concat = tf.keras.layers.Concatenate()([model_nn.output, model_hw.output])\n",
    "#concat = tf.keras.layers.multiply([model_nn.output, model_hw.output])\n",
    "concat = tf.keras.layers.multiply([output_nn, model_hw.output])\n",
    "concat = tf.keras.layers.Concatenate()([concat,output_nn, model_hw.output])\n",
    "\n",
    "output = tf.keras.layers.Dense(units=32, activation='relu')(concat)\n",
    "output = tf.keras.layers.Dense(units=32, activation='relu')(output)\n",
    "output = tf.keras.layers.Dense(units=1, activation='relu')(output)\n",
    "#full_model = tf.keras.Model(inputs=[input_nn, model_hw.input], outputs=[output])\n",
    "full_model = tf.keras.Model(inputs=[input_blocks, input_nn, model_hw.input], outputs=[output])\n",
    "Wsave =full_model.get_weights()\n",
    "\n",
    "print(f'PARAMS : {full_model.count_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target_power, predicted_power):\n",
    "  return tf.math.abs((target_power - predicted_power)/target_power)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=nb_training_batches,\n",
    "    decay_rate=0.95)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PARAMS : 15809\nEpoch 1/20\n106/106 [==============================] - 28s 264ms/step - loss: 0.2973 - val_loss: 0.2649\nEpoch 2/20\n106/106 [==============================] - 32s 306ms/step - loss: 0.2640 - val_loss: 0.2416\nEpoch 3/20\n106/106 [==============================] - 27s 250ms/step - loss: 0.2647 - val_loss: 0.2551\nEpoch 4/20\n106/106 [==============================] - 32s 299ms/step - loss: 0.2478 - val_loss: 0.2359\nEpoch 5/20\n106/106 [==============================] - 23s 217ms/step - loss: 0.2243 - val_loss: 0.2289\nEpoch 6/20\n106/106 [==============================] - 27s 257ms/step - loss: 0.2104 - val_loss: 0.2133\nEpoch 7/20\n106/106 [==============================] - 24s 223ms/step - loss: 0.1989 - val_loss: 0.2072\nEpoch 8/20\n106/106 [==============================] - 23s 214ms/step - loss: 0.1892 - val_loss: 0.2011\nEpoch 9/20\n106/106 [==============================] - 23s 216ms/step - loss: 0.1853 - val_loss: 0.2044\nEpoch 10/20\n106/106 [==============================] - 23s 217ms/step - loss: 0.1784 - val_loss: 0.1944\nEpoch 11/20\n106/106 [==============================] - 27s 258ms/step - loss: 0.1800 - val_loss: 0.1950\nEpoch 12/20\n106/106 [==============================] - 25s 236ms/step - loss: 0.1738 - val_loss: 0.1881\nEpoch 13/20\n106/106 [==============================] - 23s 213ms/step - loss: 0.1693 - val_loss: 0.1839\nEpoch 14/20\n106/106 [==============================] - 22s 207ms/step - loss: 0.1604 - val_loss: 0.1853\nEpoch 15/20\n106/106 [==============================] - 22s 206ms/step - loss: 0.1575 - val_loss: 0.1844\nEpoch 16/20\n106/106 [==============================] - 22s 208ms/step - loss: 0.1540 - val_loss: 0.1821\nEpoch 17/20\n106/106 [==============================] - 29s 270ms/step - loss: 0.1536 - val_loss: 0.1784\nEpoch 18/20\n106/106 [==============================] - 24s 227ms/step - loss: 0.1549 - val_loss: 0.1767\nEpoch 19/20\n106/106 [==============================] - 25s 239ms/step - loss: 0.1521 - val_loss: 0.1766\nEpoch 20/20\n106/106 [==============================] - 26s 243ms/step - loss: 0.1498 - val_loss: 0.1763\nTraining Full Model for 20 epochs : 17.46221411666667 min\n"
    }
   ],
   "source": [
    "#%pycache\n",
    "\n",
    "full_epochs = 20\n",
    "\n",
    "print(f'PARAMS : {full_model.count_params()}')\n",
    "t4= time.clock()\n",
    "full_model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "history = full_model.fit(train_dataset_norm,\n",
    "         validation_data=(test_dataset_norm),\n",
    "        epochs=full_epochs,\n",
    "         verbose=1,\n",
    " )\n",
    "\n",
    "t5= time.clock()\n",
    "\n",
    "\n",
    "print(f'Training Full Model for {full_epochs} epochs : {(t5-t4)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv1D object at 0x1a7a1b6780> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x1a7a1bb668>).\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x1a7a1b8400> and <tensorflow.python.keras.layers.core.Lambda object at 0x1a7a1b8390>).\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x1a7a1bec50> and <tensorflow.python.keras.layers.core.Lambda object at 0x1a7a1b83c8>).\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x1a7a1bbf98> and <tensorflow.python.keras.layers.merge.Multiply object at 0x1a7a1bb828>).\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x1a7a1e18d0> and <tensorflow.python.keras.layers.core.Dense object at 0x1a7a1bbf98>).\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1a7a1bd7f0>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "%pycache\n",
    "model_path = \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/results_best_models/model_1_12161_param_0.131_error/model_1_plus_12161_param\"\n",
    "full_model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6779374]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "X_list = [X_val_nn_norm, X_val_hw_param_norm]\n",
    "inputs2=[]\n",
    "for x in range(len (X_list)):    \n",
    "    input_x=np.array(X_list[x])[0]\n",
    "    input_x=input_x.reshape((1,*input_x.shape))\n",
    "    inputs2.append(tf.convert_to_tensor(input_x))\n",
    "full_model(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6779374]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "  # hw_array = np.array([ 0.8552576 , -0.47734305, -0.62680401,  0.57911723,  0.37841259,\n",
    "  #    -0.23434365, -0.64634273,  0.68297377,  0.06787059, -1.01696994,\n",
    "  #    -0.81343443, -0.49667088])\n",
    "  hw_array = np.array([ 10,100000,100000,10000,0,0,0,0,0,0,0,0])\n",
    "  hw_array = hw_array.reshape((1, *hw_array.shape))\n",
    "  hw_array = tf.convert_to_tensor(hw_array, dtype=np.float32)\n",
    "\n",
    "full_model([nn_array_2, hw_array])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 param\n",
    "## 25 epochs :  41 min\n",
    "\n",
    "# 12 000 param\n",
    "## 25 epochs: 30 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10,785 params:\n",
    "### 25 epochs : 56 min\n",
    "### 15 epochs : 34 min\n",
    "</br>\n",
    "\n",
    "## 31 000\n",
    "### **20 epochs : 25 min**\n",
    " </br>\n",
    "\n",
    "## 57,009 params\n",
    "### 20 epochs : 101 min \n",
    "</br>\n",
    "\n",
    "##62,465 pames\n",
    "### 30 epochs : 78 min\n",
    " </br>\n",
    "\n",
    "## 143,345 params\n",
    "### 20 epochs : 276 min \n",
    "</br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PARAMS : 4897\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-4dd477f3e7fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#X_list = [X_val_hw_param_norm, X_val_nn_norm]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_entire_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_final_epochs_for_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_final_epochs_for_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#savetxt(f'{path_entire_model}/{name}/std.csv', std, delimiter=',')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/single_path_nas/single-path-nas/HAS/plot_figures.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(path, model, history, X_list, y, std, name, nb_predictions, max_val_loss, nb_final_epochs_for_mean, save)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Directory already exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_middle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mplot_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_middle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_high\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{path}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/single_path_nas/single-path-nas/HAS/plot_figures.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, X_list, y, nb_predictions)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0my_pred_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0my_pred_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0my_pred_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "#%pycache\n",
    "\n",
    "t5= time.clock()\n",
    "nb_final_epochs_for_mean = 5\n",
    "print(f'PARAMS : {full_model.count_params()}')\n",
    "name = f'{name_test}_{full_model.count_params()}_param'\n",
    "save = True\n",
    "\n",
    "nb_predictions = 1\n",
    "max_val_loss=0.7\n",
    "\n",
    "\n",
    "\n",
    "model = full_model\n",
    "X_val_list = [X_val_nn_norm, X_val_hw_param_norm]\n",
    "history = history\n",
    "\n",
    "#model = full_model\n",
    "#X_list = [X_val_hw_param_norm, X_val_nn_norm]\n",
    "\n",
    "save_model(path=path_entire_model, model= model, history=history, X_list=X_val_list, y=y_val, std=std, name=name, nb_predictions = nb_predictions, max_val_loss=max_val_loss, nb_final_epochs_for_mean = nb_final_epochs_for_mean, save=save)\n",
    "\n",
    "#savetxt(f'{path_entire_model}/{name}/std.csv', std, delimiter=',')\n",
    "\n",
    "\n",
    "t6= time.clock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total time : 0.11341751666667127 min\n"
    }
   ],
   "source": [
    "#%pycache\n",
    "print(f'Total time : {(t6-t5)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x12ef48cf8>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "#%pycache\n",
    "checkpoint_path = \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/results_best_models/multiply/with_inversed_hw/model_1_plus_12161_param_0.131_error/model_1_plus_12161_param\"\n",
    "\n",
    "\n",
    "full_model.load_weights(checkpoint_path)\n",
    "#results = full_model.evaluate(test_dataset_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test size dataset - More data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PARAMS : 12161\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_val_nn_norm' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9c4b2b7f2c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnb_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmax_val_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_val_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_val_nn_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_hw_param_norm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val_nn_norm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#%pycache\n",
    "path_test_dataset = \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/results_best_models/size_dataset/test_size_dataset_4\"\n",
    "\n",
    "print(f'PARAMS : {full_model.count_params()}')\n",
    "nb_final_epochs_for_mean = 5\n",
    "save = True\n",
    "nb_predictions = 100\n",
    "max_val_loss=50\n",
    "X_val_list = [X_val_nn_norm, X_val_hw_param_norm]\n",
    "\n",
    "\n",
    "full_epochs = 25\n",
    "test_performances = {}\n",
    "#percents = [0.97]\n",
    "percents = [0.25, 0.50, 0.75, 0.85, 0.90, 1]\n",
    "mini_batches_list = l = [int(x * nb_training_batches) for x in percents]\n",
    "\n",
    "for i, mini_batches in enumerate(mini_batches_list):\n",
    "    print()\n",
    "    full_model.set_weights(Wsave)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=nb_training_batches,\n",
    "    decay_rate=0.95)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    train_dataset_percent_norm =train_dataset_norm.take(mini_batches)\n",
    "    t4= time.clock()\n",
    "    full_model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    history = full_model.fit(train_dataset_percent_norm,\n",
    "            validation_data=(test_dataset_norm),\n",
    "            epochs=full_epochs,\n",
    "            verbose=1,\n",
    "    )\n",
    "\n",
    "    t5= time.clock()\n",
    "    print(f'Mini_batch percent: {percents[i]} Training Full Model for {full_epochs} epochs : {(t5-t4)/60} min')\n",
    "    name = f'percent_{percents[i]}'\n",
    "    test_performances[percents[i]] =  np.mean((history.history[\"val_loss\"][-nb_final_epochs_for_mean:]))\n",
    "\n",
    "    save_model(path=path_test_dataset, model= full_model, history=history, X_list=X_val_list, y=y_val, name=name, nb_predictions = nb_predictions, max_val_loss=max_val_loss, nb_final_epochs_for_mean = nb_final_epochs_for_mean, save=save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performances_save = {0.25: 0.2629273136456808,\n",
    " 0.5: 0.23724876874023018,\n",
    " 0.75: 0.21560097336769105,\n",
    " 0.85: 0.19926969276534187,\n",
    " 0.87: 0.18293529417779708,\n",
    " 0.9: 0.17390226572752,\n",
    " 0.95: 0.1793243298927943,\n",
    " 0.97: 0.18094,\n",
    " 1: 0.1743503441413244}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nconv1d_input (InputLayer)       [(None, 37, 7)]      0                                            \n__________________________________________________________________________________________________\nconv1d (Conv1D)                 (None, 37, 128)      1920        conv1d_input[0][0]               \n__________________________________________________________________________________________________\ndense_input (InputLayer)        [(None, 12)]         0                                            \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 37, 32)       8224        conv1d[0][0]                     \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 32)           416         dense_input[0][0]                \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 32)           0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           1056        dense[0][0]                      \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 32)           0           lambda[0][0]                     \n                                                                 dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 16)           528         multiply[0][0]                   \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            17          dense_2[0][0]                    \n==================================================================================================\nTotal params: 12,161\nTrainable params: 12,161\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_performances' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d6ada02d6004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_performances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sorted by key, return a list of tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unpack a list of pairs into two tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_performances' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "lists = sorted(test_performances.items()) # sorted by key, return a list of tuples\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.title('Influence size train dataset')\n",
    "plt.ylabel('Mean error on last epochs')\n",
    "plt.xlabel('Percent train dataset')\n",
    "plt.legend(['test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "high_powers= [128,256,512,1024]\n",
    "low_power = [16,32,64]\n",
    "depths = [1,2,3,4,5,6]\n",
    "begining \n",
    "\n",
    "def loss(target_power, predicted_power):\n",
    "  return tf.math.abs((target_power - predicted_power)/target_power)\n",
    "\n",
    "for ind,power in enumerate(powers):\n",
    "    for depth in depths[:len(depths)-ind]:\n",
    "        model_nn = tf.keras.Sequential()\n",
    "        model_nn.add(layers.Conv1D(power, (3), activation='relu', padding=\"same\",input_shape=(max_blocks, nb_param)))\n",
    "        for i in range(depth):\n",
    "            model_nn.add(layers.Conv1D(power, (3), activation='relu', padding=\"same\"))\n",
    "        model_nn.add(layers.Lambda( lambda x: K.sum(x, axis=1)))\n",
    "        \n",
    "        model_hw = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(nb_hw_param,)),\n",
    "        layers.Dense(power, activation='linear')\n",
    "        ])\n",
    "\n",
    "        concat = tf.keras.layers.multiply([model_nn.output, model_hw.output])\n",
    "        output = tf.keras.layers.Dense(units=16, activation='relu')(concat)\n",
    "        output = tf.keras.layers.Dense(units=1, activation='relu')(output)\n",
    "        full_model = tf.keras.Model(inputs=[model_nn.input, model_hw.input], outputs=[output])\n",
    "       \n",
    "        # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        # initial_learning_rate=1e-2,\n",
    "        # decay_steps=nb_training_batches,\n",
    "        # decay_rate=0.95)\n",
    "        # optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        # full_epochs = 1\n",
    "\n",
    "        # print(f'PARAMS : {full_model.count_params()}')\n",
    "        # t4= time.clock()\n",
    "        # full_model.compile(optimizer=optimizer, loss=loss)\n",
    "        # history = full_model.fit(train_dataset_norm,\n",
    "        #         validation_data=(test_dataset_norm),\n",
    "        #         epochs=full_epochs,\n",
    "        #         verbose=1, )\n",
    "        # t5= time.clock()\n",
    "        # print(f'Training Full Model for {full_epochs} epochs : {(t5-t4)/60} min')\n",
    "        # t5= time.clock()\n",
    "        # nb_final_epochs_for_mean = 5\n",
    "        # print(f'PARAMS : {full_model.count_params()}')\n",
    "        name = f'power_{power}_depth_{depth}_{full_model.count_params()}_param'\n",
    "        print(nb_repeat)\n",
    "        #print(depth)\n",
    "        print()\n",
    "        # save = True\n",
    "\n",
    "        # nb_predictions = 100\n",
    "        # max_val_loss=0.7\n",
    "\n",
    "        # model = full_model\n",
    "        # X_val_list = [X_val_nn_norm, X_val_hw_param_norm]\n",
    "\n",
    "        # save_model(path=path_entire_model, model= model, history=history, X_list=X_val_list, y=y_val, std=std, name=name, nb_predictions = nb_predictions, max_val_loss=max_val_loss, nb_final_epochs_for_mean = nb_final_epochs_for_mean, save=save)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bithascondabd8f1537e5d34ba799ef15b8707e6526",
   "display_name": "Python 3.6.10 64-bit ('has': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}