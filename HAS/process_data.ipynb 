{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import glob, os\n",
    "from numpy import savetxt, loadtxt, asarray\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)  Encompass all training data (resp. val) in one csv file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the Neural Networks Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_nets = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/DataForHASpredictor2/all_nets.json'\n",
    "more_nets = '/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/DataForHASpredictor2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to training and validation files. \n",
    "#### If not already converted, will be under the \"txt\" extension (see bellow). Otherwise, alread in the \"csv\" format.\n",
    "\n",
    "#### It may not contain all the data (some was already treated), but will be concatened to the already treated csv files at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_nondups_val2 = \"\"\n",
    "parsed_nondups_train2 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ONLY HAVE TO DO IT ONCE) Convert txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "number = 2\n",
    "\n",
    "with open(parsed_nondups_val2, 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open(f'parsed_nondups_val{number}.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerows(lines)\n",
    "\n",
    "with open(parsed_nondups_train2, 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open(f'parsed_nondups_train{number}.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerows(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other way to treat Additional data : encompass txt files to a global csv file. Not used if a csv file alread exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_00001600-00001649.txt\ntrain_00001850-00001899.txt\ntrain_00002000-00002049.txt\ntrain_00002300-00002349.txt\ntrain_00002200-00002249.txt\ntrain_00001950-00001999.txt\ntrain_00002100-00002149.txt\ntrain_00001700-00001749.txt\ntrain_00001750-00001799.txt\ntrain_00002250-00002299.txt\ntrain_00002150-00002199.txt\ntrain_00001900-00001949.txt\ntrain_00002050-00002099.txt\ntrain_00001800-00001849.txt\ntrain_00002350-00002399.txt\ntrain_00001650-00001699.txt\n"
    }
   ],
   "source": [
    "#%pycache\n",
    "os.chdir(more_nets)\n",
    "add_train = glob.glob(\"train*.txt\")\n",
    "add_val =  glob.glob(\"val*.txt\")\n",
    "\n",
    "with open(f'new_parsed_train.csv', 'w') as out_file:\n",
    "    c=0\n",
    "    writer = csv.writer(out_file)\n",
    "    for train in add_train:\n",
    "        print(train)\n",
    "        with open(more_nets +'/'+ train, 'r') as in_file:\n",
    "            stripped = (line.strip() for line in in_file)\n",
    "            lines = (line.split(\",\") for line in stripped if line)\n",
    "            writer.writerows(lines)\n",
    "\n",
    "\n",
    "with open(f'new_parsed_val.csv', 'w') as out_file:\n",
    "    c=0\n",
    "    for val in add_val:\n",
    "        with open(more_nets +'/'+ val, 'r') as in_file:\n",
    "            stripped = (line.strip() for line in in_file  )\n",
    "            lines = (line.split(\",\") for line in stripped if line)\n",
    "            writer = csv.writer(out_file)\n",
    "            writer.writerows(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once all training and validation data have been concatened, those paths define the entire csv to be treated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parsed_nondups_train_csv = \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/DataForHASpredictor2/new_parsed_train.csv\"\n",
    "\n",
    "\n",
    "parsed_nondups_val_csv = \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/DataForHASpredictor2/new_parsed_val.csv\"\n",
    "\n",
    "\n",
    "\n",
    "#parsed_nondups_val_csv = '*/new_parsed_val.csv'\n",
    "#parsed_nondups_train_csv = '*/new_parsed_train.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hw = pd.read_csv(parsed_nondups_train_csv)\n",
    "val_hw = pd.read_csv(parsed_nondups_val_csv)\n",
    "\n",
    "\n",
    "train_hw = train_hw[train_hw.C != 'C']\n",
    "val_hw = val_hw[val_hw.C != 'C']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              C          B      name                   G  \\\n2531   748323.0  2679043.0  00001619   8141295.936344964   \n2532  6129302.0  7236639.0  00001619  1698060.5572549982   \n2533  6015364.0  7236639.0  00001619  1757963.4398911092   \n\n                          J mac_num max_filter_size mac_array_num  \\\n2531  9.865678969799997e-05    65.0          4098.0           7.0   \n2532  8.896639770600013e-05    28.0           512.0           2.0   \n2533  8.506332450000004e-05    41.0           512.0           2.0   \n\n             total_power   bw_power          core_power data_bits sram_size  \n2531  14.662443587919999  10.716172  3.9462715879199988    2048.0  379904.0  \n2532   32.50521190824001  28.946556  3.5586559082400053     128.0   80000.0  \n2533  32.349088980000005  28.946556  3.4025329800000015     128.0   80000.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C</th>\n      <th>B</th>\n      <th>name</th>\n      <th>G</th>\n      <th>J</th>\n      <th>mac_num</th>\n      <th>max_filter_size</th>\n      <th>mac_array_num</th>\n      <th>total_power</th>\n      <th>bw_power</th>\n      <th>core_power</th>\n      <th>data_bits</th>\n      <th>sram_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2531</th>\n      <td>748323.0</td>\n      <td>2679043.0</td>\n      <td>00001619</td>\n      <td>8141295.936344964</td>\n      <td>9.865678969799997e-05</td>\n      <td>65.0</td>\n      <td>4098.0</td>\n      <td>7.0</td>\n      <td>14.662443587919999</td>\n      <td>10.716172</td>\n      <td>3.9462715879199988</td>\n      <td>2048.0</td>\n      <td>379904.0</td>\n    </tr>\n    <tr>\n      <th>2532</th>\n      <td>6129302.0</td>\n      <td>7236639.0</td>\n      <td>00001619</td>\n      <td>1698060.5572549982</td>\n      <td>8.896639770600013e-05</td>\n      <td>28.0</td>\n      <td>512.0</td>\n      <td>2.0</td>\n      <td>32.50521190824001</td>\n      <td>28.946556</td>\n      <td>3.5586559082400053</td>\n      <td>128.0</td>\n      <td>80000.0</td>\n    </tr>\n    <tr>\n      <th>2533</th>\n      <td>6015364.0</td>\n      <td>7236639.0</td>\n      <td>00001619</td>\n      <td>1757963.4398911092</td>\n      <td>8.506332450000004e-05</td>\n      <td>41.0</td>\n      <td>512.0</td>\n      <td>2.0</td>\n      <td>32.349088980000005</td>\n      <td>28.946556</td>\n      <td>3.4025329800000015</td>\n      <td>128.0</td>\n      <td>80000.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_hw[2531:2534]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Dictionnary - Old + New nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2449"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "\n",
    "with open(all_nets, 'r') as f:\n",
    "    nets_dict = json.load(f)\n",
    "\n",
    "\n",
    "os.chdir(more_nets)\n",
    "all_filenames = glob.glob(\"*.json\")\n",
    "for file in all_filenames:\n",
    "    with open(more_nets +'/'+ file, 'r') as f:\n",
    "        file_dict = json.load(f)\n",
    "        nets_dict.update(file_dict)\n",
    "len(nets_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Normalize some colmuns to ease researchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (NOT USED) Filter power under a certain value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "val_hw[\"total_power\"]=pd.to_numeric(val_hw[\"total_power\"], errors='coerce')\n",
    "train_hw[\"total_power\"]=pd.to_numeric(train_hw[\"total_power\"], errors='coerce')\n",
    "\n",
    "\n",
    "train_hw = train_hw[(train_hw.total_power) < 200]\n",
    "val_hw = val_hw[(val_hw.total_power) < 200 ]\n",
    "\n",
    "\n",
    "train_hw.to_csv( \"parsed_nondups_train_lower_power.csv\", index=False, encoding='utf-8-sig')\n",
    "val_hw.to_csv( \"parsed_nondups_val_lower_power.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dictionnary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_zeros(k):\n",
    "    \"Normalize the name in order to find them in the training (resp. val) files\"\n",
    "    nb = len(k)\n",
    "    return \"0\"*(8-nb)\n",
    "\n",
    "\n",
    "def f(mydict):\n",
    "    return dict((number_zeros(k)+k,f(v) if hasattr(v,'keys') else v) for k,v in mydict.items())\n",
    "\n",
    "nets_dict = f(nets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "'00002449' in nets_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['00000000', '00000001', '00000002', ..., '00002447', '00002448',\n       '00002449'], dtype='<U8')"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "np.sort(list(nets_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the training (resp val) files \"name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    00001642\n1    00001642\n2    00001642\n3    00001642\n4    00001642\nName: name, dtype: object"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "def convert_NN_name(x):\n",
    "    x=int(x)\n",
    "    x=str(x)\n",
    "    while len(str(x))<8 :\n",
    "        x = '0'+str(x)\n",
    "    return x\n",
    "\n",
    "train_hw['name']=train_hw['name'].apply(lambda x : convert_NN_name(x))\n",
    "val_hw['name']=val_hw['name'].apply(lambda x : convert_NN_name(x))\n",
    "\n",
    "train_hw[\"name\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max block size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Later used to normalize the input shapes for all the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_blocks = 0\n",
    "for k,nn in nets_dict.items():\n",
    "    if len(nn)>max_blocks:\n",
    "        max_blocks=len(nn)\n",
    "\n",
    "max_blocks = max_blocks +1 # add fc layer at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "37"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "max_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add neural network to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hw[\"NN\"]=train_hw['name'].map(nets_dict)\n",
    "val_hw[\"NN\"]=val_hw['name'].map(nets_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Treat Neural Networks : Calculation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate flops - depending of the convtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME DIDNT COUNT ACTIVATION\n",
    "\n",
    "def conv_flops(hout,cin,cout,k,tensor_in,skip=0):\n",
    "    flops = 2*k*k*cin*hout*hout*cout\n",
    "    if skip ==1:\n",
    "        flops += tensor_in\n",
    "    return flops\n",
    "\n",
    "def dw_flops(hout,c,k,tensor_in,skip=0, mult=1):\n",
    "    flops = 2*k*k*c*hout*hout\n",
    "    if skip == 1:\n",
    "        flops +=tensor_in\n",
    "    return flops\n",
    "\n",
    "def fcc_flops(i, j):\n",
    "    return (2*i-1)*j\n",
    "\n",
    "def calculate_flop(convtype,hin,hout,cin,cout,k,exp,tensor_in, skip=0, mult=1,skip_op=False):\n",
    "    if skip_op==True:\n",
    "        return 0\n",
    "    else:\n",
    "        if convtype=='conv':\n",
    "            return conv_flops(hout,cin,cout,k, tensor_in, skip)\n",
    "        elif convtype=='dw':\n",
    "            return dw_flops(hout,cin,k,tensor_in,skip)\n",
    "        elif convtype=='inv':\n",
    "            hout_pointwise = hin\n",
    "            return conv_flops(hout=hout_pointwise, cin=cin,cout=cin*exp,k=1, tensor_in=tensor_in, skip=skip) + \\\n",
    "        dw_flops(hout=hout,c=cin*exp,k=k,tensor_in=0,skip=0) + \\\n",
    "        conv_flops(hout=hout,cin=cin*exp,cout=cout,k=1, tensor_in=0, skip=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Calculate weights - depending of the convtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_weights(cin,cout,k):\n",
    "    return (k*k*cin+1)*cout\n",
    "\n",
    "def dw_weights(cin,k,mult=1):\n",
    "    #dw = equi cin=1 and multiply by cout\n",
    "    #return conv_flops(hout,1,cin,k)*mult\n",
    "    return (k*k+1)*cin\n",
    "\n",
    "def fcc_weight(i, j):\n",
    "    return\n",
    "    #return (2*i-1)*j\n",
    "\n",
    "def calculate_weights(convtype,cin,cout,k,exp,mult=1):\n",
    "    if convtype=='conv':\n",
    "        return conv_weights(cin,cout,k)\n",
    "    elif convtype=='dw':\n",
    "        return dw_weights(cin,k)\n",
    "    elif convtype=='inv':\n",
    "        return conv_weights(cin, cin*exp, 1)+ dw_weights(cin*exp,k,mult) + conv_weights(cin*exp,cout,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (NOT USED) Distinguish columns between convtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distinct_contypes(NN_frame, columns_names, conv_types):\n",
    "    columns_names_augmented = [ f'{name}_{conv_type}' for conv_type in conv_types for name in columns_names]\n",
    "    NN_frame_processed= pd.DataFrame(None, index=np.arange(len(NN)+1), columns=columns_names_augmented)\n",
    "\n",
    "    ### Len(NN)+1  beceause +1 FC layer at the end\n",
    "    for conv_type in conv_types:\n",
    "        columns_conv_type = [f'{columns_name}_{conv_type}' for columns_name in columns_names]\n",
    "        NN_frame_processed[columns_conv_type] = NN_frame.loc[NN_frame['convtype']==conv_type, columns_names]\n",
    "    return NN_frame_processed.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat the neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_classes= 1000\n",
    "separate_types = False\n",
    "\n",
    "# def no_relu(NN_frame):  \n",
    "#     NN_frame['convtype']= NN_frame['convtype'].apply(lambda x : 'conv' if x=='conv_norelu' else x)\n",
    "\n",
    "def treat_NN(NN_frame, values_to_keep, separate_types):\n",
    "    fc_block = {'exp':1, 'c_out':1000, 's':1, 'k':1, 'skip':0,'convtype':'conv'}\n",
    "    NN_frame=NN_frame.append(fc_block, ignore_index=True)\n",
    "    import pdb\n",
    "   # pdb.set_trace()\n",
    "\n",
    "    #no_relu\n",
    "    NN_frame['convtype']= NN_frame['convtype'].apply(lambda x : 'conv' if x=='conv_norelu' else x)\n",
    "\n",
    "    NN_frame['c_in'] = np.roll(NN_frame['c_out'], 1)\n",
    "    NN_frame['c_in'][0]=3 # RGB channels\n",
    "\n",
    "    NN_frame['k2']=NN_frame['k'].apply(lambda x : x*x)\n",
    "    NN_frame['hidden_dim']=NN_frame['exp']*NN_frame['c_in']\n",
    "\n",
    "    NN_frame['h_in'] = np.nan\n",
    "    NN_frame['h_in'][0]=224 # Size imagenet - hardcoded\n",
    "    for i in range(1, len(NN_frame)):\n",
    "        if NN_frame.loc[i-1, 's']==1:\n",
    "            NN_frame.loc[i, 'h_in']=NN_frame.loc[i-1, 'h_in']\n",
    "        elif NN_frame.loc[i-1, 's']==2:\n",
    "            NN_frame.loc[i, 'h_in']=NN_frame.loc[i-1, 'h_in']/2\n",
    "        else : \n",
    "            raise NameError('Dont know the paddind')\n",
    "\n",
    "\n",
    "    NN_frame['h_out'] = np.roll(NN_frame['h_in'], -1)\n",
    "    NN_frame['h_out'][NN_frame.last_valid_index()]=NN_frame['h_in'][NN_frame.last_valid_index()]/NN_frame.loc[NN_frame.last_valid_index(), 's']\n",
    "\n",
    "    NN_frame['tensor_in']=NN_frame['c_in']*NN_frame['h_in']*NN_frame['h_in']\n",
    "    NN_frame['tensor_out']=NN_frame['c_out']*NN_frame['h_out']*NN_frame['h_out']\n",
    "\n",
    "    NN_frame['weights']=NN_frame.apply(lambda x : calculate_weights(x.convtype,x.c_in,x.c_out,x.k,x.exp), axis=1)\n",
    "    NN_frame['FLOPS']= NN_frame.apply(lambda x : calculate_flop(x.convtype,x.h_in,x.h_out,x.c_in,x.c_out,x.k,x.exp, x.tensor_in,x.skip), axis=1)\n",
    "    #return  NN_frame\n",
    "    if separate_types:\n",
    "        NN_frame=NN_frame.loc[:, values_to_keep + ['convtype']]\n",
    "        NN_frame =distinct_contypes(NN_frame, values_to_keep, conv_types)\n",
    "        #values_to_keep = [f'{value}_{type}' for type in conv_types for value in values_to_keep]\n",
    "    else:\n",
    "        NN_frame =NN_frame.loc[:, values_to_keep]\n",
    "    return  NN_frame\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example NN_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          FLOPS  weights  tensor_in  tensor_out  hidden_dim  k2  skip\n0    23708160.0      980   150528.0    439040.0           3   9     0\n1     7902720.0      350   439040.0    439040.0          35   9     0\n2    29854720.0     1224   439040.0    426496.0          35   1     0\n3   181260800.0    11909   426496.0     28224.0         170  25     0\n4     7620480.0     1314    28224.0     28224.0          45   9     0\n5     3951360.0     1406    28224.0      8624.0          45   9     0\n6     2673440.0     1826     8624.0      8624.0          55   9     0\n7     2121504.0     3379     8624.0      2548.0          66  25     0\n8     2293200.0     6019     2548.0      2548.0          78  49     0\n9     2293200.0     6019     2548.0      2548.0          78  49     0\n10    1070160.0     2899     2548.0      2548.0          78   9     0\n11     347802.0     2136     2548.0      1470.0          39   9     0\n12     610050.0     6420     1470.0      1470.0          90   9     1\n13     599760.0     6329     1470.0      1421.0          90   9     0\n14    3066518.0    32370     1421.0     52871.0          29   1     0\n15  105742000.0  1080000    52871.0     49000.0        1079   1     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FLOPS</th>\n      <th>weights</th>\n      <th>tensor_in</th>\n      <th>tensor_out</th>\n      <th>hidden_dim</th>\n      <th>k2</th>\n      <th>skip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23708160.0</td>\n      <td>980</td>\n      <td>150528.0</td>\n      <td>439040.0</td>\n      <td>3</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7902720.0</td>\n      <td>350</td>\n      <td>439040.0</td>\n      <td>439040.0</td>\n      <td>35</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29854720.0</td>\n      <td>1224</td>\n      <td>439040.0</td>\n      <td>426496.0</td>\n      <td>35</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>181260800.0</td>\n      <td>11909</td>\n      <td>426496.0</td>\n      <td>28224.0</td>\n      <td>170</td>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7620480.0</td>\n      <td>1314</td>\n      <td>28224.0</td>\n      <td>28224.0</td>\n      <td>45</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3951360.0</td>\n      <td>1406</td>\n      <td>28224.0</td>\n      <td>8624.0</td>\n      <td>45</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2673440.0</td>\n      <td>1826</td>\n      <td>8624.0</td>\n      <td>8624.0</td>\n      <td>55</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2121504.0</td>\n      <td>3379</td>\n      <td>8624.0</td>\n      <td>2548.0</td>\n      <td>66</td>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2293200.0</td>\n      <td>6019</td>\n      <td>2548.0</td>\n      <td>2548.0</td>\n      <td>78</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2293200.0</td>\n      <td>6019</td>\n      <td>2548.0</td>\n      <td>2548.0</td>\n      <td>78</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1070160.0</td>\n      <td>2899</td>\n      <td>2548.0</td>\n      <td>2548.0</td>\n      <td>78</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>347802.0</td>\n      <td>2136</td>\n      <td>2548.0</td>\n      <td>1470.0</td>\n      <td>39</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>610050.0</td>\n      <td>6420</td>\n      <td>1470.0</td>\n      <td>1470.0</td>\n      <td>90</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>599760.0</td>\n      <td>6329</td>\n      <td>1470.0</td>\n      <td>1421.0</td>\n      <td>90</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3066518.0</td>\n      <td>32370</td>\n      <td>1421.0</td>\n      <td>52871.0</td>\n      <td>29</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>105742000.0</td>\n      <td>1080000</td>\n      <td>52871.0</td>\n      <td>49000.0</td>\n      <td>1079</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "conv_types = ['conv', 'dw', 'inv']\n",
    "columns_names = ['exp','c_out','s','k','skip']\n",
    "values_to_keep = ['FLOPS','weights','tensor_in','tensor_out','hidden_dim','k2', 'skip']\n",
    "#values_to_keep_plus_type = ['FLOPS','weights','tensor_in','tensor_out','hidden_dim','k2', 'skip', 'convtype']\n",
    "\n",
    "\n",
    "\n",
    "NN = nets_dict['00000001']\n",
    "NN_frame = pd.DataFrame(NN, columns = columns_names+ ['convtype',])\n",
    "\n",
    "\n",
    "NN_00001 = treat_NN(NN_frame, values_to_keep, False)\n",
    "NN_00001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the final representation of the NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          FLOPS  weights  tensor_in  tensor_out  hidden_dim  k2  skip\n0    23708160.0      980   150528.0    439040.0           3   9     0\n1     7902720.0      350   439040.0    439040.0          35   9     0\n2    29854720.0     1224   439040.0    426496.0          35   1     0\n3   181260800.0    11909   426496.0     28224.0         170  25     0\n4     7620480.0     1314    28224.0     28224.0          45   9     0\n5     3951360.0     1406    28224.0      8624.0          45   9     0\n6     2673440.0     1826     8624.0      8624.0          55   9     0\n7     2121504.0     3379     8624.0      2548.0          66  25     0\n8     2293200.0     6019     2548.0      2548.0          78  49     0\n9     2293200.0     6019     2548.0      2548.0          78  49     0\n10    1070160.0     2899     2548.0      2548.0          78   9     0\n11     347802.0     2136     2548.0      1470.0          39   9     0\n12     610050.0     6420     1470.0      1470.0          90   9     1\n13     599760.0     6329     1470.0      1421.0          90   9     0\n14    3066518.0    32370     1421.0     52871.0          29   1     0\n15  105742000.0  1080000    52871.0     49000.0        1079   1     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FLOPS</th>\n      <th>weights</th>\n      <th>tensor_in</th>\n      <th>tensor_out</th>\n      <th>hidden_dim</th>\n      <th>k2</th>\n      <th>skip</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23708160.0</td>\n      <td>980</td>\n      <td>150528.0</td>\n      <td>439040.0</td>\n      <td>3</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7902720.0</td>\n      <td>350</td>\n      <td>439040.0</td>\n      <td>439040.0</td>\n      <td>35</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29854720.0</td>\n      <td>1224</td>\n      <td>439040.0</td>\n      <td>426496.0</td>\n      <td>35</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>181260800.0</td>\n      <td>11909</td>\n      <td>426496.0</td>\n      <td>28224.0</td>\n      <td>170</td>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7620480.0</td>\n      <td>1314</td>\n      <td>28224.0</td>\n      <td>28224.0</td>\n      <td>45</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3951360.0</td>\n      <td>1406</td>\n      <td>28224.0</td>\n      <td>8624.0</td>\n      <td>45</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2673440.0</td>\n      <td>1826</td>\n      <td>8624.0</td>\n      <td>8624.0</td>\n      <td>55</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2121504.0</td>\n      <td>3379</td>\n      <td>8624.0</td>\n      <td>2548.0</td>\n      <td>66</td>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2293200.0</td>\n      <td>6019</td>\n      <td>2548.0</td>\n      <td>2548.0</td>\n      <td>78</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2293200.0</td>\n      <td>6019</td>\n      <td>2548.0</td>\n      <td>2548.0</td>\n      <td>78</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1070160.0</td>\n      <td>2899</td>\n      <td>2548.0</td>\n      <td>2548.0</td>\n      <td>78</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>347802.0</td>\n      <td>2136</td>\n      <td>2548.0</td>\n      <td>1470.0</td>\n      <td>39</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>610050.0</td>\n      <td>6420</td>\n      <td>1470.0</td>\n      <td>1470.0</td>\n      <td>90</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>599760.0</td>\n      <td>6329</td>\n      <td>1470.0</td>\n      <td>1421.0</td>\n      <td>90</td>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3066518.0</td>\n      <td>32370</td>\n      <td>1421.0</td>\n      <td>52871.0</td>\n      <td>29</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>105742000.0</td>\n      <td>1080000</td>\n      <td>52871.0</td>\n      <td>49000.0</td>\n      <td>1079</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "\n",
    "def no_relu(NN_frame):  \n",
    "    NN_frame['convtype']= NN_frame['convtype'].apply(lambda x : 'conv' if x=='conv_norelu' else x)\n",
    "\n",
    "\n",
    "def NN_to_dataframe(NN_list, columns_names, conv_types, values_to_keep,separate_types):\n",
    "    NN_frame = pd.DataFrame(NN_list, columns = columns_names+ ['convtype',])\n",
    "    no_relu(NN_frame)\n",
    "    return treat_NN(NN_frame, values_to_keep,separate_types)\n",
    "\n",
    "\n",
    "NN_df = NN_to_dataframe(NN, columns_names, conv_types,values_to_keep,separate_types)\n",
    "NN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[1, 35, 2, 3, 0, 'conv'],\n [1, 35, 1, 3, 0, 'dw'],\n [1, 34, 1, 1, 0, 'conv_norelu'],\n [5, 9, 2, 5, 0, 'inv'],\n [5, 9, 1, 3, 0, 'inv'],\n [5, 11, 2, 3, 0, 'inv'],\n [5, 11, 1, 3, 0, 'inv'],\n [6, 13, 2, 5, 0, 'inv'],\n [6, 13, 1, 7, 0, 'inv'],\n [6, 13, 1, 7, 0, 'inv'],\n [6, 13, 1, 3, 0, 'inv'],\n [3, 30, 2, 3, 0, 'inv'],\n [3, 30, 1, 3, 1, 'inv'],\n [3, 29, 1, 3, 0, 'inv'],\n [1, 1079, 1, 1, 0, 'conv']]"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (NOT USED) Add HW params to NN_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "params = ['mac_num','mac_array_num','data_bits','sram_size','max_filter_size']\n",
    "\n",
    "def add_HW_param_column(df,mac_num, mac_array_num, data_bits, sram_size, max_filter_size):\n",
    "    df['mac_num']=mac_num\n",
    "    df['mac_array_num']=mac_array_num\n",
    "    df['total_macs']=  mac_num * mac_array_num\n",
    "    df['data_bits']=data_bits\n",
    "    df['sram_size']=sram_size\n",
    "    df['max_filter_size']=max_filter_size\n",
    "\n",
    "\n",
    "def add_HW_param_line(NN_frame, mac_num, mac_array_num, data_bits, sram_size, max_filter_size):\n",
    "    HW_param_line =pd.DataFrame([[mac_num]*(NN_frame.shape[1])], columns=NN_frame.columns)\n",
    "    HW_param_line = HW_param_line.append(pd.DataFrame([[mac_array_num]*(NN_frame.shape[1])], columns=NN_frame.columns))\n",
    "    HW_param_line = HW_param_line.append(pd.DataFrame([[mac_num*mac_array_num]*(NN_frame.shape[1])], columns=NN_frame.columns))\n",
    "    HW_param_line =HW_param_line.append(pd.DataFrame([[data_bits]*(NN_frame.shape[1])], columns=NN_frame.columns))\n",
    "    HW_param_line =HW_param_line.append(pd.DataFrame([[sram_size]*(NN_frame.shape[1])], columns=NN_frame.columns))\n",
    "    HW_param_line =HW_param_line.append(pd.DataFrame([[max_filter_size]*(NN_frame.shape[1])], columns=NN_frame.columns))\n",
    "    return HW_param_line.append(NN_frame)\n",
    "\n",
    "\n",
    "#not_used\n",
    "def extend_to_max_blocks(NN_frame, value):\n",
    "    for i in range (max_blocks-len(NN_frame)):\n",
    "        NN_frame=NN_frame.append(pd.DataFrame([[0]*(NN_frame.shape[1])], index=[i+len(NN_frame),] ,columns=NN_frame.columns))\n",
    "    return NN_frame\n",
    "\n",
    "#not_used\n",
    "def add_zero_blocks(arr):\n",
    "    zero_blocks = np.zeros(( max_blocks-arr.shape[0],arr.shape[1]))\n",
    "    return np.append(arr, zero_blocks, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "train_hw= train_hw[:3]\n",
    "val_hw = val_hw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Apply and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "values_to_keep = ['FLOPS','weights','tensor_in','tensor_out','hidden_dim','k2', 'skip']\n",
    "separate_types = False\n",
    "\n",
    "name = 'from_1600'\n",
    "\n",
    "# Convert NN to dataframe with values of interest\n",
    "train_hw[\"NN_dataframe\"] = train_hw[\"NN\"].progress_apply(lambda x : NN_to_dataframe(x,columns_names, conv_types, values_to_keep,separate_types))\n",
    "train_hw['NN_dataframe']=train_hw['NN_dataframe'].progress_apply(lambda x : np.array(x))\n",
    "train_hw['NN_dataframe'].to_csv(f'train_{name}.csv')\n",
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "val_hw[\"NN_dataframe\"] = val_hw[\"NN\"].progress_apply(lambda x : NN_to_dataframe(x,columns_names, conv_types, values_to_keep,separate_types))\n",
    "\n",
    "val_hw['NN_dataframe']=val_hw['NN_dataframe'].progress_apply(lambda x : np.array(x))\n",
    "val_hw['NN_dataframe'].to_csv(f'val_{name}.csv')\n",
    "\n",
    "#!!!! In practise the validation file was saved with more information  : val_hw.to_csv(f'val_{name}.csv') !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load to verify everythinng is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "UsageError: Line magic function `%pycache` not found.\n"
    }
   ],
   "source": [
    "%pycache\n",
    "import ast\n",
    "\n",
    "\n",
    "path_processed_val_nn=\"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/DataForHASpredictor/val_low_power.csv\"\n",
    "\n",
    "def from_np_array(array_string):\n",
    "    array_string = ','.join(array_string.replace('[ ', '[').split())\n",
    "    return np.asarray(ast.literal_eval(array_string))\n",
    "\n",
    "val = pd.read_csv(path_processed_val_nn,converters={'NN_dataframe': from_np_array})\n",
    "\n",
    "X_train_nn =np.array(val['NN_dataframe'].tolist())\n",
    "#train = pd.read_csv(path_processed_train_nn,converters={'NN_dataframe': from_np_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              0         1         2         3      4     5    6\n0    16257024.0     672.0  150528.0  301056.0    3.0   9.0  0.0\n1     5419008.0     240.0  301056.0  301056.0   24.0   9.0  0.0\n2    21676032.0     900.0  301056.0  451584.0   24.0   1.0  0.0\n3   158054400.0    9958.0  451584.0   68992.0  144.0   9.0  0.0\n4    29252608.0    4862.0   68992.0   68992.0   88.0   9.0  0.0\n5    94657024.0   15427.0   68992.0   84672.0  154.0  49.0  0.0\n6    80269056.0   13149.0   84672.0   84672.0  162.0  25.0  0.0\n7    42674688.0    7047.0   84672.0   84672.0  108.0   9.0  0.0\n8    53597376.0    8775.0   84672.0   84672.0  108.0  25.0  1.0\n9   104654592.0   17037.0   84672.0   84672.0  162.0  49.0  0.0\n10   21422016.0    3537.0   84672.0   84672.0   54.0   9.0  1.0\n11   40007520.0   14882.0   84672.0   25088.0  135.0  49.0  0.0\n12   25037824.0   16467.0   25088.0  391216.0   32.0   1.0  0.0\n13  782432000.0  500000.0  391216.0  784000.0  499.0   1.0  0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16257024.0</td>\n      <td>672.0</td>\n      <td>150528.0</td>\n      <td>301056.0</td>\n      <td>3.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5419008.0</td>\n      <td>240.0</td>\n      <td>301056.0</td>\n      <td>301056.0</td>\n      <td>24.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21676032.0</td>\n      <td>900.0</td>\n      <td>301056.0</td>\n      <td>451584.0</td>\n      <td>24.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>158054400.0</td>\n      <td>9958.0</td>\n      <td>451584.0</td>\n      <td>68992.0</td>\n      <td>144.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>29252608.0</td>\n      <td>4862.0</td>\n      <td>68992.0</td>\n      <td>68992.0</td>\n      <td>88.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>94657024.0</td>\n      <td>15427.0</td>\n      <td>68992.0</td>\n      <td>84672.0</td>\n      <td>154.0</td>\n      <td>49.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>80269056.0</td>\n      <td>13149.0</td>\n      <td>84672.0</td>\n      <td>84672.0</td>\n      <td>162.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>42674688.0</td>\n      <td>7047.0</td>\n      <td>84672.0</td>\n      <td>84672.0</td>\n      <td>108.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>53597376.0</td>\n      <td>8775.0</td>\n      <td>84672.0</td>\n      <td>84672.0</td>\n      <td>108.0</td>\n      <td>25.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>104654592.0</td>\n      <td>17037.0</td>\n      <td>84672.0</td>\n      <td>84672.0</td>\n      <td>162.0</td>\n      <td>49.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>21422016.0</td>\n      <td>3537.0</td>\n      <td>84672.0</td>\n      <td>84672.0</td>\n      <td>54.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>40007520.0</td>\n      <td>14882.0</td>\n      <td>84672.0</td>\n      <td>25088.0</td>\n      <td>135.0</td>\n      <td>49.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>25037824.0</td>\n      <td>16467.0</td>\n      <td>25088.0</td>\n      <td>391216.0</td>\n      <td>32.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>782432000.0</td>\n      <td>500000.0</td>\n      <td>391216.0</td>\n      <td>784000.0</td>\n      <td>499.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "pd.DataFrame(val['NN_dataframe'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we have treated only one part of the data, combine all the processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import pandas as pd\n",
    "os.chdir(\"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data\")\n",
    "extension = 'csv'\n",
    "\n",
    "trains = [\"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/data_180035train/parsed_nondups_train_2.csv\", \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/DataForHASpredictor2/new_parsed_train.csv\"]\n",
    "\n",
    "vals = [\"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/data_180035train/parsed_nondups_val_2.csv\", \"/Users/roxanefischer/Desktop/single_path_nas/single-path-nas/HAS/data/DataForHASpredictor2/new_parsed_val.csv\"]\n",
    "#all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "\n",
    "#combine all files in the list\n",
    "#train_csv = pd.concat([pd.read_csv(f) for f in trains])\n",
    "#val_csv = pd.concat([pd.read_csv(f) for f in vals])\n",
    "\n",
    "train_l = []\n",
    "for f in trains:\n",
    "    panda = pd.read_csv(f)\n",
    "    panda = panda[panda.C != 'C']\n",
    "    panda = panda.astype(float)\n",
    "    train_l.append(panda)\n",
    "train_csv = pd.concat(train_l)\n",
    "\n",
    "\n",
    "\n",
    "val_l = []\n",
    "for f in vals:\n",
    "    panda = pd.read_csv(f)\n",
    "    panda = panda[panda.C != 'C']\n",
    "    panda = panda.astype(float)\n",
    "    val_l.append(panda)\n",
    "val_csv = pd.concat(val_l)\n",
    "\n",
    "\n",
    "\n",
    "#train_hw = train_hw[train_hw.C != 'C']\n",
    "#export to csv\n",
    "train_csv.to_csv( \"parsed_nondups_train_3.csv\", index=False, encoding='utf-8-sig')\n",
    "val_csv.to_csv( \"parsed_nondups_val_3.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Éditer les Méta-Données",
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('has': conda)",
   "language": "python",
   "name": "python361064bithascondabd8f1537e5d34ba799ef15b8707e6526"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}